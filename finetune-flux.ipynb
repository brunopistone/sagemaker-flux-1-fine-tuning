{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune FLUX.1-schnell with SageMaker Distributed Data Parallel (SMDDP)\n",
    "\n",
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate how to fine-tune the FLUX.1-schnell model using Hugging Face PEFT - LoRA, bitsandbytes, with SageMaker Distrubuted Data Parallel library\n",
    "\n",
    "Fine-Tuning:\n",
    "* Instance Type: ml.p4dn.24xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required libriaries, including the Hugging Face libraries, and restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T12:38:06.851473Z",
     "start_time": "2023-07-20T12:38:04.440644Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and upload the dataset\n",
    "\n",
    "Read train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"diffusers/tuxemon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    dataset_name\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune FLUX.1-schnell on Amazon SageMaker\n",
    "\n",
    "We are now ready to fine-tune our model. The training script is located in [./scripts/train.py](./scripts/train.py).\n",
    "\n",
    "We are going to use SageMaker Distributed Data Parallel with `AllGather` as collective operation, shard the model across all the available GPUs, and `torchrun` as script launcher for distributing the training across the GPUs available in the `ml.p4d.24xlarge`\n",
    "\n",
    "For more information about SageMaker Distributed Data Parallel, please visit the official [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"black-forest-labs/FLUX.1-schnell\"\n",
    "dataset_name = \"diffusers/tuxemon\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the hyperparameters used in the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"pretrained_model_name_or_path\": model_id,\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"output_dir\": \"/opt/ml/checkpoint\",\n",
    "    \"mixed_precision\": \"bf16\",\n",
    "    \"instance_prompt\": \"describe\",\n",
    "    \"resolution\": 1024,\n",
    "    \"train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"use_8bit_adam\": True,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"lr_scheduler\": \"constant\",\n",
    "    \"lr_warmup_steps\": 0,\n",
    "    \"seed\": \"42\",\n",
    "    \"rank\": 16,\n",
    "    \"train_text_encoder\": True,\n",
    "    \"max_sequence_length\": 512,\n",
    "    \"max_train_steps\": 500,\n",
    "    \"caption_column\": \"gpt4_turbo_caption\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below estimtor will train the model with LoRA and will save the adapter in S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "job_name = f\"train-{model_id.split('/')[-1].replace('.', '-')}\"\n",
    "\n",
    "# Create SageMaker PyTorch Estimator\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point= 'train.py',\n",
    "    source_dir=\"./scripts\",\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    framework_version=\"2.2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    #disable_output_compression=True, # Avoid compression in .tar.gz\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "pytorch_estimator.fit(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
